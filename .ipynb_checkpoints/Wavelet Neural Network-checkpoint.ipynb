{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet Neural Network (Rede Neural baseada em Wavelets)\n",
    "\n",
    "## Introdução\n",
    "\n",
    "As redes neurais wavelet combinam a teoria das wavelets e as redes neurais. Uma rede neural wavelet consiste geralmente de uma rede neural de alimentação direta, com uma camada oculta, cujas funções de ativação são extraídas de uma família de wavelets.\n",
    "\n",
    "Uma das aplicações das redes neurais wavelet é a **estimativa de funções**. Dada uma série de valores observados de uma função, uma rede wavelet pode ser treinada para aprender a composição dessa função e, assim, calcular um valor esperado para uma dada entrada.\n",
    "\n",
    "## O que é uma Rede Neural Wavelet?\n",
    "\n",
    "A estrutura de uma rede neural wavelet é muito semelhante à de uma rede neural MLP. Isto é, uma rede neural de avanço, recebendo uma ou mais entradas, com uma camada oculta e cuja camada de saída consiste em um ou mais combinadores lineares. A camada oculta consiste em neurônios, cujas funções de ativação são extraídas de uma base wavelet. Esses neurônios wavelet são geralmente chamados de _wavelons_.\n",
    "\n",
    "Aqui estão duas abordagens principais para a criação de redes neurais wavelet:\n",
    "\n",
    "- Na primeira, a wavelet e o processamento da rede neural são executados separadamente. O sinal de entrada é decomposto pela primeira vez usando alguma base wavelet pelos neurônios na camada oculta. Os coeficientes wavelet são, então, utilizados para modificar (atualizar) os pesos sinápticos de entrada de acordo com algum algoritmo de aprendizado.\n",
    "\n",
    "- Na segundo, combina-se as duas teorias. Neste caso, a tradução e a dilatação das wavelets, juntamente com os pesos de de entrada, são modificadas de acordo com algum algoritmo de aprendizado.\n",
    "\n",
    "Em geral, quando a primeira abordagem é utilizada, apenas as dilatações diádicas e as traduções da wavelet mãe formam a base da wavelet. Esse tipo de rede neural wavelet é geralmente chamado de wavenet. O segundo tipo é simplesmente chamado de rede wavelet.\n",
    "\n",
    "## Características\n",
    "\n",
    "- Não possui polarização ou limiar na camada oculta nem na camada de saída;\n",
    "\n",
    "## Equações\n",
    "\n",
    "### Computação no sentido direto\n",
    "\n",
    "\\begin{equation}\n",
    "    net_j(n) = \\sum_{i = 1}^{N_i} x_i(n) \\cdotp t_{ji}\n",
    "\\end{equation}\n",
    "\n",
    "A função de ativação é a derivada segunda da sigmóide, em relação ao seu argumento $\\theta$, em que $\\theta = \\frac{net_j - t_{ji}}{r_j}$.\n",
    "\n",
    "\\begin{equation}\n",
    "    y_j = \\varphi(net_j) = \\frac{d^2}{d\\theta^2} sig(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    z = \\sum_{j = 1}^{N_h} a_j(n)y_j(n)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Computação do sentido reverso\n",
    "\n",
    "\\begin{equation}\n",
    "    E(n) = \\frac{1}{2} e(n)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    e(n) = d(n) - z(n)\n",
    "\\end{equation}\n",
    "\n",
    "### Ajuste das variáveis livres\n",
    "\n",
    "#### Ajuste dos pesos $a_j$\n",
    "\n",
    "\\begin{equation} \\label{eq:delta_aj_inicial}\n",
    "    \\Delta a_j = -\\eta\\bigtriangledown_{a_j} E = -\\eta \\frac{\\partial E}{\\partial a_j}\n",
    "\\end{equation}\n",
    "\n",
    "Utilizando a regra da cadeia, calcula-se a derivada parcial:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial E}{\\partial a_j} = \\frac{\\partial E}{\\partial e} \\cdotp \\frac{\\partial e}{\\partial z} \\cdotp \\frac{\\partial z}{\\partial a_j} = e \\cdotp (-1) \\cdotp y_j\n",
    "\\end{equation}\n",
    "\n",
    "Portanto,\n",
    "\n",
    "\\begin{equation} \\label{eq:delta_aj_final}\n",
    "    \\Delta a_j = \\eta \\cdotp e \\cdotp \\frac{d^2}{d\\theta^2} sig(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "#### Ajuste dos centros $t_{ji}$\n",
    "\n",
    "\\begin{equation} \\label{eq:delta_tji_inicial}\n",
    "    \\Delta t_{ji} = -\\eta\\bigtriangledown_{t_{ji}} E = -\\eta \\frac{\\partial E}{\\partial t_{ji}}\n",
    "\\end{equation}\n",
    "\n",
    "Utilizando a regra da cadeia, calcula-se a derivada parcial:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial E}{\\partial t_{ji}} = \\frac{\\partial E}{\\partial e} \\cdotp \\frac{\\partial e}{\\partial z} \\cdotp \\frac{\\partial z}{\\partial y_j} \\cdotp \\frac{\\partial y_j}{\\partial t_{ji}} = e \\cdotp (-1) \\cdotp a_j \\cdotp \\left[\\frac{d^3}{d\\theta^3}sig(\\theta)\\cdotp\\left(\\frac{-1}{r_j}\\right)\\right]\n",
    "\\end{equation}\n",
    "\n",
    "É válido destacar que a derivada de $y_j$ em relação a $t_{ji}$ foi feita, primeiro, derivando a sigmóide em relação à variável $\\theta$ e, em seguida, derivando a variável $\\theta$ em relação a $t_{ji}$. Ou seja, aplicou-se também a regra da cadeia.\n",
    "\n",
    "Assim, voltando à Equação do $\\Delta t_{ji}$, temos:\n",
    "\n",
    "\\begin{equation} \\label{eq:delta_tji_final}\n",
    "    \\Delta t_{ji} = -\\eta\\frac{e\\cdotp a_j}{r_j}\\cdotp \\frac{d^3}{d\\theta^3}sig(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "#### Ajuste das larguras (ou dilatações) $r_j$\n",
    "\n",
    "\\begin{equation} \\label{eq:delta_rj_inicial}\n",
    "    \\Delta r_{j} = -\\eta\\bigtriangledown_{r_{j}} E = -\\eta \\frac{\\partial E}{\\partial r_{j}}\n",
    "\\end{equation}\n",
    "\n",
    "Utilizando a regra da cadeia, calcula-se a derivada parcial:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial E}{\\partial r_{j}} = \\frac{\\partial E}{\\partial e} \\cdotp \\frac{\\partial e}{\\partial z} \\cdotp \\frac{\\partial z}{\\partial y_j} \\cdotp \\frac{\\partial y_j}{\\partial r_{j}} = e \\cdotp (-1) \\cdotp a_j \\cdotp \\left[\\frac{d^3}{d\\theta^3}sig(\\theta)\\cdotp\\left(-\\frac{(net_j - t_{ji})}{r_j^2}\\right)\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Nesse caso, também é válido destacar que a derivada de $y_j$ em relação a $r_{j}$ foi feita, primeiro, derivando a sigmóide em relação à variável $\\theta$ e, em seguida, derivando a variável $\\theta$ em relação a $r_{j}$. Ou seja, aplicou-se também a regra da cadeia.\n",
    "\n",
    "Assim, voltando à Equação do $\\Delta r_{j}$, temos:\n",
    "\n",
    "\\begin{equation} \\label{eq:delta_rj_final}\n",
    "    \\Delta r_{ji} = -\\eta\\frac{e\\cdotp a_j \\cdotp (net_j - t_{ji})}{r_j^2}\\cdotp \\frac{d^3}{d\\theta^3}sig(\\theta)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "theta = np.arange(-10, 10, 0.001)\n",
    "\n",
    "plt.plot(theta, 2*(1 / (1 + np.exp(-theta)))**3 - 3*(1 / (1 + np.exp(-theta)))**2 + (1 / (1 + np.exp(-theta))))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
